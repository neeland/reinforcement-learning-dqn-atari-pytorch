{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "RL - DQN Lab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neonkitchen/rl-dqn/blob/master/RL_DQN_Lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbmzUwjiHC3f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %load dqn/wrappers.py\n",
        "\"\"\"\n",
        "Useful wrappers taken from OpenAI (https://github.com/openai/baselines)\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import gym\n",
        "from gym import spaces\n",
        "import cv2\n",
        "\n",
        "cv2.ocl.setUseOpenCL(False)\n",
        "\n",
        "\n",
        "class NoopResetEnv(gym.Wrapper):\n",
        "    def __init__(self, env, noop_max=30):\n",
        "        \"\"\"Sample initial states by taking random number of no-ops on reset.\n",
        "        No-op is assumed to be action 0.\n",
        "        \"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        self.noop_max = noop_max\n",
        "        self.override_num_noops = None\n",
        "        self.noop_action = 0\n",
        "        assert env.unwrapped.get_action_meanings()[0] == 'NOOP'\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        \"\"\" Do no-op action for a number of steps in [1, noop_max].\"\"\"\n",
        "        self.env.reset(**kwargs)\n",
        "        if self.override_num_noops is not None:\n",
        "            noops = self.override_num_noops\n",
        "        else:\n",
        "            noops = self.unwrapped.np_random.randint(1, self.noop_max + 1)  # pylint: disable=E1101\n",
        "        assert noops > 0\n",
        "        obs = None\n",
        "        for _ in range(noops):\n",
        "            obs, _, done, _ = self.env.step(self.noop_action)\n",
        "            if done:\n",
        "                obs = self.env.reset(**kwargs)\n",
        "        return obs\n",
        "\n",
        "    def step(self, ac):\n",
        "        return self.env.step(ac)\n",
        "\n",
        "\n",
        "class FireResetEnv(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        \"\"\"Take action on reset for environments that are fixed until firing.\"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
        "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        self.env.reset(**kwargs)\n",
        "        obs, _, done, _ = self.env.step(1)\n",
        "        if done:\n",
        "            self.env.reset(**kwargs)\n",
        "        obs, _, done, _ = self.env.step(2)\n",
        "        if done:\n",
        "            self.env.reset(**kwargs)\n",
        "        return obs\n",
        "\n",
        "    def step(self, ac):\n",
        "        return self.env.step(ac)\n",
        "\n",
        "\n",
        "class EpisodicLifeEnv(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        \"\"\"Make end-of-life == end-of-episode, but only reset on true game over.\n",
        "        Done by DeepMind for the DQN and co. since it helps value estimation.\n",
        "        \"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        self.lives = 0\n",
        "        self.was_real_done = True\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, done, info = self.env.step(action)\n",
        "        self.was_real_done = done\n",
        "        # check current lives, make loss of life terminal,\n",
        "        # then update lives to handle bonus lives\n",
        "        lives = self.env.unwrapped.ale.lives()\n",
        "        if lives < self.lives and lives > 0:\n",
        "            # for Qbert sometimes we stay in lives == 0 condtion for a few frames\n",
        "            # so its important to keep lives > 0, so that we only reset once\n",
        "            # the environment advertises done.\n",
        "            done = True\n",
        "        self.lives = lives\n",
        "        return obs, reward, done, info\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        \"\"\"Reset only when lives are exhausted.\n",
        "        This way all states are still reachable even though lives are episodic,\n",
        "        and the learner need not know about any of this behind-the-scenes.\n",
        "        \"\"\"\n",
        "        if self.was_real_done:\n",
        "            obs = self.env.reset(**kwargs)\n",
        "        else:\n",
        "            # no-op step to advance from terminal/lost life state\n",
        "            obs, _, _, _ = self.env.step(0)\n",
        "        self.lives = self.env.unwrapped.ale.lives()\n",
        "        return obs\n",
        "\n",
        "\n",
        "class MaxAndSkipEnv(gym.Wrapper):\n",
        "    def __init__(self, env, skip=4):\n",
        "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        # most recent raw observations (for max pooling across time steps)\n",
        "        self._obs_buffer = np.zeros((2,) + env.observation_space.shape, dtype=np.uint8)\n",
        "        self._skip = skip\n",
        "\n",
        "    def reset(self):\n",
        "        return self.env.reset()\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Repeat action, sum reward, and max over last observations.\"\"\"\n",
        "        total_reward = 0.0\n",
        "        done = None\n",
        "        for i in range(self._skip):\n",
        "            obs, reward, done, info = self.env.step(action)\n",
        "            if i == self._skip - 2: self._obs_buffer[0] = obs\n",
        "            if i == self._skip - 1: self._obs_buffer[1] = obs\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        # Note that the observation on the done=True frame\n",
        "        # doesn't matter\n",
        "        max_frame = self._obs_buffer.max(axis=0)\n",
        "\n",
        "        return max_frame, total_reward, done, info\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        return self.env.reset(**kwargs)\n",
        "\n",
        "\n",
        "class ClipRewardEnv(gym.RewardWrapper):\n",
        "    def __init__(self, env):\n",
        "        gym.RewardWrapper.__init__(self, env)\n",
        "\n",
        "    def reward(self, reward):\n",
        "        \"\"\"Bin reward to {+1, 0, -1} by its sign.\"\"\"\n",
        "        return np.sign(reward)\n",
        "\n",
        "\n",
        "class WarpFrame(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        \"\"\"Warp frames to 84x84 as done in the Nature paper and later work.\n",
        "        Expects inputs to be of shape height x width x num_channels\n",
        "        \"\"\"\n",
        "        gym.ObservationWrapper.__init__(self, env)\n",
        "        self.width = 84\n",
        "        self.height = 84\n",
        "        self.observation_space = spaces.Box(low=0, high=255,\n",
        "                                            shape=(self.height, self.width, 1), dtype=np.uint8)\n",
        "\n",
        "    def observation(self, frame):\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
        "        frame = cv2.resize(frame, (self.width, self.height), interpolation=cv2.INTER_AREA)\n",
        "        return frame[:, :, None]\n",
        "\n",
        "\n",
        "class FrameStack(gym.Wrapper):\n",
        "    def __init__(self, env, k):\n",
        "        \"\"\"Stack k last frames.\n",
        "        Returns lazy array, which is much more memory efficient.\n",
        "        Expects inputs to be of shape num_channels x height x width.\n",
        "        \"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        self.k = k\n",
        "        self.frames = deque([], maxlen=k)\n",
        "        shp = env.observation_space.shape\n",
        "        self.observation_space = spaces.Box(low=0, high=255, shape=(shp[0] * k, shp[1], shp[2]), dtype=np.uint8)\n",
        "\n",
        "    def reset(self):\n",
        "        ob = self.env.reset()\n",
        "        for _ in range(self.k):\n",
        "            self.frames.append(ob)\n",
        "        return self._get_ob()\n",
        "\n",
        "    def step(self, action):\n",
        "        ob, reward, done, info = self.env.step(action)\n",
        "        self.frames.append(ob)\n",
        "        return self._get_ob(), reward, done, info\n",
        "\n",
        "    def _get_ob(self):\n",
        "        assert len(self.frames) == self.k\n",
        "        return LazyFrames(list(self.frames))\n",
        "\n",
        "\n",
        "class ScaledFloatFrame(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        gym.ObservationWrapper.__init__(self, env)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        # careful! This undoes the memory optimization, use\n",
        "        # with smaller replay buffers only.\n",
        "        return np.array(observation).astype(np.float32) / 255.0\n",
        "\n",
        "\n",
        "class LazyFrames(object):\n",
        "    def __init__(self, frames):\n",
        "        \"\"\"This object ensures that common frames between the observations are only stored once.\n",
        "        It exists purely to optimize memory usage which can be huge for DQN's 1M frames replay\n",
        "        buffers.\"\"\"\n",
        "        self._frames = frames\n",
        "\n",
        "    def __array__(self, dtype=None):\n",
        "        out = np.concatenate(self._frames, axis=0)\n",
        "        if dtype is not None:\n",
        "            out = out.astype(dtype)\n",
        "        return out\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._frames)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self._frames[i]\n",
        "\n",
        "\n",
        "class PyTorchFrame(gym.ObservationWrapper):\n",
        "    \"\"\"Image shape to num_channels x height x width\"\"\"\n",
        "\n",
        "    def __init__(self, env):\n",
        "        super(PyTorchFrame, self).__init__(env)\n",
        "        shape = self.observation_space.shape\n",
        "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(shape[-1], shape[0], shape[1]), dtype=np.uint8)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return np.rollaxis(observation, 2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7OoLEloUwAb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AFyqJERHC3q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %load dqn/replay_buffer.py\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class ReplayBuffer:\n",
        "    \"\"\"\n",
        "    Simple storage for transitions from an environment.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, size):\n",
        "        \"\"\"\n",
        "        Initialise a buffer of a given size for storing transitions\n",
        "        :param size: the maximum number of transitions that can be stored\n",
        "        \"\"\"\n",
        "        self._storage = []\n",
        "        self._maxsize = size\n",
        "        self._next_idx = 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._storage)\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        \"\"\"\n",
        "        Add a transition to the buffer. Old transitions will be overwritten if the buffer is full.\n",
        "        :param state: the agent's initial state\n",
        "        :param action: the action taken by the agent\n",
        "        :param reward: the reward the agent received\n",
        "        :param next_state: the subsequent state\n",
        "        :param done: whether the episode terminated\n",
        "        \"\"\"\n",
        "        data = (state, action, reward, next_state, done)\n",
        "\n",
        "        if self._next_idx >= len(self._storage):\n",
        "            self._storage.append(data)\n",
        "        else:\n",
        "            self._storage[self._next_idx] = data\n",
        "        self._next_idx = (self._next_idx + 1) % self._maxsize\n",
        "\n",
        "    def _encode_sample(self, indices):\n",
        "        states, actions, rewards, next_states, dones = [], [], [], [], []\n",
        "        for i in indices:\n",
        "            data = self._storage[i]\n",
        "            state, action, reward, next_state, done = data\n",
        "            states.append(np.array(state, copy=False))\n",
        "            actions.append(action)\n",
        "            rewards.append(reward)\n",
        "            next_states.append(np.array(next_state, copy=False))\n",
        "            dones.append(done)\n",
        "        return np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(dones)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"\n",
        "        Randomly sample a batch of transitions from the buffer.\n",
        "        :param batch_size: the number of transitions to sample\n",
        "        :return: a mini-batch of sampled transitions\n",
        "        \"\"\"\n",
        "        indices = np.random.randint(0, len(self._storage) - 1, size=batch_size)\n",
        "        return self._encode_sample(indices)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXLs58XRHC3s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %load dqn/model.py\n",
        "from gym import spaces\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# Class inheritance example with Pytorch, can use Tensorflow instead.\n",
        "class DQN(nn.Module):\n",
        "    \"\"\"\n",
        "    A basic implementation of a Deep Q-Network. The architecture is the same as that described in the\n",
        "    Nature DQN paper.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 observation_space: spaces.Box,\n",
        "                 action_space: spaces.Discrete):\n",
        "        \"\"\"\n",
        "        Initialise the DQN\n",
        "        :param observation_space: the state space of the environment\n",
        "        :param action_space: the action space of the environment\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        assert type(observation_space) == spaces.Box, 'observation_space must be of type Box'\n",
        "        assert len(observation_space.shape) == 3, 'observation space must have the form channels x width x height'\n",
        "        assert type(action_space) == spaces.Discrete, 'action_space must be of type Discrete'\n",
        "\n",
        "        # TODO Implement CNN layers\n",
        "        #raise NotImplementedError\n",
        "        #adapted from https://github.com/raillab/dqn/\n",
        "        self.conv1 = nn.Conv2d(observation_space.shape[0], 32, 8, stride=4)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 4, stride=2)\n",
        "        self.conv3 = nn.Conv2d(64, 64, 3, stride=1)\n",
        "        self.linear1 = nn.Linear(64 * 7 * 7, 512)\n",
        "        self.linear2 = nn.Linear(512, action_space.n)\n",
        "       \n",
        "        \"\"\"\n",
        "        Paper describes architecture as follows:\n",
        "        -input: 84*84*4 image \n",
        "        -first hidden layer: convolves 32 filters of 8*8, stride 4, then ReLU\n",
        "        -second hidden layer: convolves 64 filters of 4*4, stride 2, then ReLU\n",
        "        -third convolutional layer: convolves 64 filters of 3*3, stride 1, then ReLU\n",
        "        -final hidden layer: is fully-connected and consists of 512 rectifier units\n",
        "        -output layer: fully-connected linear layer -- single output for each action\n",
        "        \"\"\"\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # TODO Implement forward pass\n",
        "        #raise NotImplementedError\n",
        "        #adapted from https://github.com/raillab/dqn/\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = x.view(-1, 64 * 7 * 7)  # flatten\n",
        "        x = F.relu(self.linear1(x))\n",
        "        x = self.linear2(x)\n",
        "        return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHui-ZSfHC3v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %load dqn/agent.py\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from gym import spaces\n",
        "from torch.optim import Optimizer\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "\n",
        "#from dqn.model import DQN\n",
        "#from dqn.replay_buffer import ReplayBuffer\n",
        "\n",
        "device = \"cuda\"\n",
        "\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self,\n",
        "                 observation_space: spaces.Box,\n",
        "                 action_space: spaces.Discrete,\n",
        "                 replay_buffer: ReplayBuffer,\n",
        "                 use_double_dqn,\n",
        "                 lr,\n",
        "                 batch_size,\n",
        "                 gamma):\n",
        "        \"\"\"\n",
        "        Initialise the DQN algorithm using the Adam optimiser\n",
        "        :param action_space: the action space of the environment\n",
        "        :param observation_space: the state space of the environment\n",
        "        :param replay_buffer: storage for experience replay\n",
        "        :param lr: the learning rate for Adam\n",
        "        :param batch_size: the batch size\n",
        "        :param gamma: the discount factor\n",
        "        \"\"\"\n",
        "\n",
        "        self.memory = replay_buffer\n",
        "        self.batch_size = batch_size\n",
        "        self.use_double_dqn = use_double_dqn\n",
        "        self.gamma = gamma\n",
        "        self.policy_network = DQN(observation_space, action_space).to(device)\n",
        "        self.target_network = DQN(observation_space, action_space).to(device)\n",
        "        self.update_target_network()\n",
        "        self.target_network.eval()\n",
        "        self.optimiser =  torch.optim.Adam(self.policy_network.parameters(), lr=lr) # TODO Initialise Pytorch/Tensorflow optimiser with learning rate and policy_network.parameters()\n",
        "\n",
        "    def optimise_td_loss(self):\n",
        "        \"\"\"\n",
        "        Optimise the TD-error over a single minibatch of transitions\n",
        "        :return: the loss\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "        #   Optimise the TD-error over a single minibatch of transitions\n",
        "        #   Sample the minibatch from the replay-memory (randomly sampled)\n",
        "        #   using done (as a float) instead of if statement\n",
        "        #   return loss\n",
        "\n",
        "        #raise NotImplementedError\n",
        "        #adapted from https://github.com/raillab/dqn/\n",
        "      \n",
        "        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)\n",
        "        states = np.array(states) / 255.0\n",
        "        next_states = np.array(next_states) / 255.0\n",
        "        states = torch.from_numpy(states).float().to(device)\n",
        "        actions = torch.from_numpy(actions).long().to(device)\n",
        "        rewards = torch.from_numpy(rewards).float().to(device)\n",
        "        next_states = torch.from_numpy(next_states).float().to(device)\n",
        "        dones = torch.from_numpy(dones).float().to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if self.use_double_dqn:\n",
        "                _, max_next_action = self.policy_network(next_states).max(1)\n",
        "                max_next_q_values = self.target_network(next_states).gather(1, max_next_action.unsqueeze(1)).squeeze()\n",
        "            else:\n",
        "                next_q_values = self.target_network(next_states)\n",
        "                max_next_q_values, _ = next_q_values.max(1)\n",
        "            target_q_values = rewards + (1 - dones) * self.gamma * max_next_q_values\n",
        "\n",
        "        input_q_values = self.policy_network(states)\n",
        "        input_q_values = input_q_values.gather(1, actions.unsqueeze(1)).squeeze()\n",
        "\n",
        "        loss = F.smooth_l1_loss(input_q_values, target_q_values)\n",
        "\n",
        "        self.optimiser.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimiser.step()\n",
        "        del states\n",
        "        del next_states\n",
        "        return loss.item()\n",
        "    \n",
        "        \n",
        "            \n",
        "        \n",
        "\n",
        "    def update_target_network(self):\n",
        "        \"\"\"\n",
        "        Update the target Q-network by copying the weights from the current Q-network\n",
        "        \"\"\"\n",
        "        # TODO update target_network parameters with policy_network parameters\n",
        "        #raise NotImplementedError\n",
        "        self.target_network.load_state_dict(self.policy_network.state_dict())                                          \n",
        "       \n",
        "    def act(self, state: np.ndarray):\n",
        "        \"\"\"\n",
        "        Select an action greedily from the Q-network given the state\n",
        "        :param state: the current state\n",
        "        :return: the action to take\n",
        "        \"\"\"\n",
        "        # TODO Select action greedily from the Q-network given the state\n",
        "        #raise NotImplementedError\n",
        "        state = np.array(state) / 255.0\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            q_values = self.policy_network(state)\n",
        "            _, action = q_values.max(1)\n",
        "            return action.item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLqbVrASHC3z",
        "colab_type": "code",
        "outputId": "4619d56d-78ba-4df0-bdba-bd78eb1e3e24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# %load train_atari.py\n",
        "import random\n",
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "#from dqn.agent import DQNAgent\n",
        "#from dqn.replay_buffer import ReplayBuffer\n",
        "#from dqn.wrappers import *\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    hyper_params = {\n",
        "        \"seed\": 42,  # which seed to use\n",
        "        \"env\": \"PongNoFrameskip-v4\",  # name of the game\n",
        "        \"replay-buffer-size\": int(5e3),  # replay buffer size\n",
        "        \"learning-rate\": 1e-4,  # learning rate for Adam optimizer\n",
        "        \"discount-factor\": 0.99,  # discount factor\n",
        "        \"num-steps\": int(1e6),  # total number of steps to run the environment for\n",
        "        \"batch-size\": 32,  # number of transitions to optimize at the same time\n",
        "        \"learning-starts\": 10000,  # number of steps before learning starts\n",
        "        \"learning-freq\": 1,  # number of iterations between every optimization step\n",
        "        \"use-double-dqn\": True,  # use double deep Q-learning\n",
        "        \"target-update-freq\": 1000,  # number of iterations between every target network update\n",
        "        \"eps-start\": 1.0,  # e-greedy start threshold\n",
        "        \"eps-end\": 0.01,  # e-greedy end threshold\n",
        "        \"eps-fraction\": 0.1,  # fraction of num-steps\n",
        "        \"print-freq\": 10\n",
        "    }\n",
        "\n",
        "    np.random.seed(hyper_params[\"seed\"])\n",
        "    random.seed(hyper_params[\"seed\"])\n",
        "\n",
        "    assert \"NoFrameskip\" in hyper_params[\"env\"], \"Require environment with no frameskip\"\n",
        "    env = gym.make(hyper_params[\"env\"])\n",
        "    env.seed(hyper_params[\"seed\"])\n",
        "    \n",
        "    # TODO Pick Gym wrappers to use\n",
        "    #adapted from https://github.com/raillab/dqn/\n",
        "    env = NoopResetEnv(env, noop_max=30)\n",
        "    env = MaxAndSkipEnv(env, skip=4)\n",
        "    env = EpisodicLifeEnv(env)\n",
        "    env = FireResetEnv(env)\n",
        "    env = WarpFrame(env)\n",
        "    env = PyTorchFrame(env)\n",
        "    env = ClipRewardEnv(env)\n",
        "    env = FrameStack(env, 4)\n",
        "    env = gym.wrappers.Monitor(env, './video/', video_callable=lambda episode_id: episode_id % 10 == 0, force=True)\n",
        "    \n",
        "    \n",
        "    replay_buffer = ReplayBuffer(hyper_params[\"replay-buffer-size\"])\n",
        "\n",
        "    # TODO Create dqn agent\n",
        "    #adapted from https://github.com/raillab/dqn/\n",
        "    agent = DQNAgent(\n",
        "                 env.observation_space,\n",
        "                 env.action_space,\n",
        "                 replay_buffer,\n",
        "                 use_double_dqn= hyper_params[\"use-double-dqn\"],\n",
        "                 lr=hyper_params[\"learning-rate\"],\n",
        "                 batch_size=hyper_params[\"batch-size\"],\n",
        "                 gamma=hyper_params[\"discount-factor\"])\n",
        "\n",
        "    eps_timesteps = hyper_params[\"eps-fraction\"] * float(hyper_params[\"num-steps\"])\n",
        "    episode_rewards = [0.0]\n",
        "    loss = [0.0]\n",
        "\n",
        "    state = env.reset()\n",
        "    for t in range(hyper_params[\"num-steps\"]):\n",
        "        fraction = min(1.0, float(t) / eps_timesteps)\n",
        "        eps_threshold = hyper_params[\"eps-start\"] + fraction * (hyper_params[\"eps-end\"] - hyper_params[\"eps-start\"])\n",
        "        sample = random.random()\n",
        "        # TODO\n",
        "        #adapted from https://github.com/raillab/dqn/ \n",
        "        #  select random action if sample is less equal than eps_threshold\n",
        "        if sample > eps_threshold:\n",
        "            action = agent.act(np.array(state))\n",
        "        else:\n",
        "            action = env.action_space.sample()\n",
        "\n",
        "        # take step in env      \n",
        "        next_state, reward, done, _ =  env.step(action)     \n",
        "        # add state, action, reward, next_state, float(done) to reply memory - cast done to float\n",
        "        agent.memory.add(state, action, reward, next_state, float(done))\n",
        "        state = next_state\n",
        "        # add reward to episode_reward \n",
        "        episode_rewards[-1] += reward\n",
        "        if done:\n",
        "            state = env.reset()\n",
        "            episode_rewards.append(0.0)\n",
        "\n",
        "        if t > hyper_params[\"learning-starts\"] and t % hyper_params[\"learning-freq\"] == 0:\n",
        "            agent.optimise_td_loss()\n",
        "\n",
        "        if t > hyper_params[\"learning-starts\"] and t % hyper_params[\"target-update-freq\"] == 0:\n",
        "            agent.update_target_network()\n",
        "\n",
        "        num_episodes = len(episode_rewards)\n",
        "\n",
        "        if done and hyper_params[\"print-freq\"] is not None and len(episode_rewards) % hyper_params[\n",
        "            \"print-freq\"] == 0:\n",
        "            mean_100ep_reward = round(np.mean(episode_rewards[-101:-1]), 1)\n",
        "            print(\"********************************************************\")\n",
        "            print(\"steps: {}\".format(t))\n",
        "            print(\"episodes: {}\".format(num_episodes))\n",
        "            print(\"mean 100 episode reward: {}\".format(mean_100ep_reward))\n",
        "            print(\"% time spent exploring: {}\".format(int(100 * eps_threshold)))\n",
        "            print(\"********************************************************\")\n",
        "            np.savetxt('rewards.csv', episode_rewards)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "********************************************************\n",
            "steps: 8449\n",
            "episodes: 10\n",
            "mean 100 episode reward: -20.3\n",
            "% time spent exploring: 91\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 17263\n",
            "episodes: 20\n",
            "mean 100 episode reward: -20.4\n",
            "% time spent exploring: 82\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 26505\n",
            "episodes: 30\n",
            "mean 100 episode reward: -20.3\n",
            "% time spent exploring: 73\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 35206\n",
            "episodes: 40\n",
            "mean 100 episode reward: -20.3\n",
            "% time spent exploring: 65\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 45239\n",
            "episodes: 50\n",
            "mean 100 episode reward: -20.3\n",
            "% time spent exploring: 55\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 54714\n",
            "episodes: 60\n",
            "mean 100 episode reward: -20.2\n",
            "% time spent exploring: 45\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 63744\n",
            "episodes: 70\n",
            "mean 100 episode reward: -20.2\n",
            "% time spent exploring: 36\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 74006\n",
            "episodes: 80\n",
            "mean 100 episode reward: -20.1\n",
            "% time spent exploring: 26\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 86569\n",
            "episodes: 90\n",
            "mean 100 episode reward: -20.1\n",
            "% time spent exploring: 14\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 102431\n",
            "episodes: 100\n",
            "mean 100 episode reward: -19.7\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 118340\n",
            "episodes: 110\n",
            "mean 100 episode reward: -19.3\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 135581\n",
            "episodes: 120\n",
            "mean 100 episode reward: -19.1\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 154566\n",
            "episodes: 130\n",
            "mean 100 episode reward: -18.5\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 174650\n",
            "episodes: 140\n",
            "mean 100 episode reward: -17.9\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 201851\n",
            "episodes: 150\n",
            "mean 100 episode reward: -16.7\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 222252\n",
            "episodes: 160\n",
            "mean 100 episode reward: -16.2\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 244660\n",
            "episodes: 170\n",
            "mean 100 episode reward: -15.4\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 275040\n",
            "episodes: 180\n",
            "mean 100 episode reward: -14.0\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 306936\n",
            "episodes: 190\n",
            "mean 100 episode reward: -11.8\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 338176\n",
            "episodes: 200\n",
            "mean 100 episode reward: -10.4\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 367872\n",
            "episodes: 210\n",
            "mean 100 episode reward: -8.3\n",
            "% time spent exploring: 1\n",
            "********************************************************\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}